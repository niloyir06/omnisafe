## Copyright 2023 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# PPOLagSem configuration: PPO-Lagrangian with optional semantic (CLIP/SigLIP) shaping & risk head.
# Mirrors PPOLag.yaml fields 1:1 plus an expanded semantic_cfgs block. All extra parameters are
# strictly additive (disabled by default) so baseline parity is preserved when enable=False.

defaults:
  # seed for random number generator
  seed: 0
  train_cfgs:
    # device to use for training, options: cpu, cuda, cuda:0, cuda:0,1, etc.
    device: cpu
    # number of threads for torch (lower when many env processes to avoid oversubscription)
    torch_threads: 1
    # number of vectorized environments
    vector_env_nums: 1
    # number of parallel agents (A3C-style); keep 1 unless multi-agent replication implemented
    parallel: 1
    # total number of steps to train
    total_steps: 10000000
  algo_cfgs:
    # number of steps to roll out before each update (per epoch)
    steps_per_epoch: 20000
    # number of PPO epochs (passes) per batch
    update_iters: 40
    # mini-batch size per update iteration
    batch_size: 64
    # target KL divergence threshold for early stopping
    target_kl: 0.02
    # entropy bonus coefficient
    entropy_coef: 0.0
    # normalize reward signal
    reward_normalize: False
    # normalize cost signal
    cost_normalize: False
    # normalize observations
    obs_normalize: True
    # stop updating early if KL > target_kl
    kl_early_stop: True
    # enable gradient clipping by global norm
    use_max_grad_norm: True
    # gradient clipping threshold
    max_grad_norm: 40.0
    # apply L2 regularization to critic parameters
    use_critic_norm: True
    # coefficient for critic L2 norm penalty
    critic_norm_coef: 0.001
    # reward discount factor
    gamma: 0.99
    # cost discount factor
    cost_gamma: 0.99
    # lambda for reward GAE
    lam: 0.95
    # lambda for cost GAE
    lam_c: 0.95
    # PPO clip ratio
    clip: 0.2
    # advantage estimation method (gae|retrace)
    adv_estimation_method: gae
    # standardize reward advantages
    standardized_rew_adv: True
    # standardize cost advantages
    standardized_cost_adv: True
    # penalty coefficient (unused in pure Lagrangian but kept for compatibility)
    penalty_coef: 0.0
    # whether to treat cost constraint explicitly
    use_cost: True
  logger_cfgs:
    # use wandb for logging
    use_wandb: False
    # wandb project name
    wandb_project: omnisafe
    # use tensorboard for logging
    use_tensorboard: True
    # save model frequency (epochs)
    save_model_freq: 100
    # logging output directory
    log_dir: "./runs"
    # rolling window for episodic stats
    window_lens: 100
  model_cfgs:
    # weight initialization mode
    weight_initialization_mode: "kaiming_uniform"
    # actor type (gaussian | gaussian_learning)
    actor_type: gaussian_learning
    # linear learning rate decay over training
    linear_lr_decay: True
    # exploration noise anneal (for gaussian_learning)
    exploration_noise_anneal: False
    # initial and final std range for gaussian_learning actor
    std_range: [0.5, 0.1]
    actor:
      # hidden layer sizes
      hidden_sizes: [64, 64]
      # activation function
      activation: tanh
      # learning rate
      lr: 0.0003
    critic:
      # hidden layer sizes
      hidden_sizes: [64, 64]
      # activation function
      activation: tanh
      # learning rate
      lr: 0.0003
  lagrange_cfgs:
    # tolerance (upper bound) on expected discounted cost
    cost_limit: 25.0
    # initial Lagrange multiplier value
    lagrangian_multiplier_init: 0.001
    # learning rate for multiplier update
    lambda_lr: 0.035
    # optimizer used for multiplier parameter
    lambda_optimizer: "Adam"
  env_cfgs: {}
  semantic_cfgs:
    # master enable switch for all semantic features. When False: no vision-language model is loaded,
    # no extra memory/time overhead, rewards are untouched (parity with baseline PPOLag).
    enable: False
    # capture interval (in env steps) between embedding extractions. Lower = denser shaping & risk data
    # but higher wall-clock overhead. Effective semantic steps ≈ total_steps / capture_interval.
    capture_interval: 4
    # side length (pixels) each frame is resized/cropped to before CLIP. Must match model expectations.
    # Smaller custom values reduce accuracy; 224 is the standard for ViT-B/16 & B/32 variants.
    frame_size: 224
    # HuggingFace model identifier for the vision-language encoder. Supports CLIP or SigLIP family models.
    # Patch size / resolution trades speed vs fidelity (e.g., clip-vit-base-patch16 vs siglip-so400m-patch14-384).
    # Changing this invalidates previously cached centroids; prompts will be re-embedded automatically.
    # Examples:
    #   CLIP   : openai/clip-vit-base-patch16
    #   SigLIP : google/siglip2-so400m-patch16-512
    model_name: "google/siglip2-so400m-patch16-512"
    # device on which the (frozen) vision-language model runs. Use a GPU (cuda:0) for large capture frequency or many envs.
      # If GPU not available it falls back to CPU automatically (status logged internally).
    model_device: cpu
    # device where semantic centroids, pooled embeddings, and risk buffers live. Keeping this on CPU saves VRAM
    # at the cost of extra host<->device transfers if the risk head is on GPU. Set to same GPU as model for
    # maximum throughput when memory allows.
    host_device: cpu
    # enable semantic reward shaping (additive or potential per potential_enable). Uses embedding margin between
    # safe and unsafe prompt centroids with annealed beta schedule. Disabled -> environment reward only.
    shaping_enable: False
    # enable auxiliary risk prediction head (embedding -> truncated discounted future cost). Provides extra
    # representation signal and powers optional modulation. Adds one small MLP optimizer step per PPO epoch.
    risk_enable: False
    # learning rate for risk head optimizer (Adam). Tune lower if loss diverges; higher if loss plateaus quickly.
    risk_lr: 0.001
    # when True: discounted cost accumulation for targets resets at episode terminals, preventing leakage of
    # post-terminal costs backward into previous episodes (crucial for multi-episode buffers).
    risk_episode_mask_enable: True
    # enable adaptive scaling (currently conservative damping) of Lagrange multiplier learning rate using
    # distributional statistics of recent risk predictions (quantile gap logistic gate).
    modulation_enable: False
    # minimum number of fully completed episodes before modulation is allowed (simple data sufficiency gate
    # to avoid early noisy scaling). Until reached, modulation scale = 1.0.
    modulation_min_episodes: 10
    # initial (maximum) shaping coefficient beta_0. Governs early influence of semantic signal; interacts with
    # normalization & margin_scale. Too high risks over-shaping; too low yields minimal curriculum effect.
    beta_start: 0.15
  # fraction of total training steps over which beta cosine-anneals from beta_start -> 0. Ensures shaping
  # neutrality late in training. Larger fraction prolongs guidance; smaller fraction decays earlier.
    beta_end_step_fraction: 0.6
    # prediction horizon (steps) used to build truncated discounted cost targets for the risk head. Larger horizon
    # increases variance; smaller horizon increases bias (misses longer-tail costs).
    risk_horizon: 64
    # discount factor used specifically for constructing risk targets (can mirror cost_gamma). Adjust (slightly)
    # lower than reward gamma to emphasize near-term safety if needed.
    discount: 0.99
    # minimum number of buffered embeddings before attempting any risk head update (data sufficiency gate)
    risk_min_samples: 5
    # optional mini-batch size for risk head (0 = use full available batch). Tune for stochastic regularization vs speed.
    risk_batch_size: 0
    # number of risk head update iterations per PPO epoch (>=1). With mini-batching runs multiple sampled steps.
    risk_update_iters: 1
    # modulation alpha: scales the strength of logistic gate effect on λ learning rate. Higher -> stronger damping
    # (lower final scale) given same gate activation.
    alpha_modulation: 2.0
    # percentile (0-100) of risk prediction distribution used as reference threshold q_p. Contrasted with mean to
    # form gap driving logistic gate. Mid-percentiles (50-70) usually stable.
    threshold_percentile: 60
    # slope divisor controlling logistic gate sharpness; effective temperature ~ q_p / slope. Larger slope softens
    # response; smaller slope sharpens (risk of instability if too small).
    slope: 5.0
    # maximum number of (embedding, cost, done) entries retained for risk training & modulation stats. Caps memory
    # and compute. When full, oldest entries are evicted FIFO.
    window_size: 2048
    # number of recent margins tracked for running mean/std when normalization enabled. Larger = smoother but slower
    # to adapt if prompt distribution shifts; smaller = noisier normalization.
    norm_window: 1000
    # enable z-normalization (subtract running mean & divide by running std) before clipping to [-2,2]. Disabling
    # leaves raw (scaled) margin; use margin_scale then to control magnitude to avoid extreme shaping.
    margin_norm_enable: True
    # multiplicative factor applied to raw margin pre-normalization & pre-clamp. Use >1 to amplify weak semantic
    # separation or <1 to temper strong margins before normalization/clipping.
    margin_scale: 1.0
    # switch shaping formulation: True -> potential-based (beta * (gamma*phi_next - phi_prev)) offering theoretical
    # policy invariance as beta→0; False -> additive (beta * clipped_norm_margin) which can speed very early learning.
    potential_enable: True
    # list of natural language prompts describing *safe / desirable* visual states. Encoded once to a centroid.
    # Edit to refine semantic signal (e.g., add context, objects, verbs). Keep concise & discriminative.
    safe_prompts: [
        "red car moving toward green goal",
        "clear path ahead to green goal",
        "red car aligned safely avoiding obstacles",
    ]
    # list of prompts describing *unsafe / undesirable* states (collisions, hazards). Provide diverse failure modes
    # without redundancy. Removing weak or ambiguous prompts can tighten margin distribution.
    unsafe_prompts: [
        "red car near purple obstacle collision",
        "red car pushing into blue cube hazard",
        "blocked path crowded with purple obstacles",
    ]
    # if True: collect one frame per parallel env at each capture step and embed as a single batch (better GPU
    # utilization & per-env fidelity). If False: legacy single frame path (embedding broadcast across envs).
    batch_across_envs: True
    # upper bound on batch size for a capture event (protects VRAM). If vector_env_nums exceeds this value frames
    # beyond the cap are ignored this capture.
    batch_max: 32
    # when True: on CUDA Out-Of-Memory during batch embedding, recursively halves the batch until success (>=1) or
    # gives up (marking failures). Prevents hard crash on transient memory pressure.
    oom_backoff: True
    # Set batch_across_envs False to revert to legacy single-frame embedding path.
    # temporal_window: size of rolling window (in capture events) for temporal mean pooling of embeddings.
    # 1 disables temporal pooling (current frame only). For k>1 we maintain last k embeddings and use their mean
    # as the effective embedding for shaping (warm-up uses mean over available <k). Suggested small values (e.g., 4–8)
    # can reduce per-frame semantic noise without large delay. Episode boundaries clear the window.
    temporal_window: 6