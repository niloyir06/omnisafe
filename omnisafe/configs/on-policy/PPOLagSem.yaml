# PPOLagSem config (copy-minimal of PPOLag for semantic variant)

defaults:
  seed: 0
  train_cfgs:
    device: cpu
    torch_threads: 16
    vector_env_nums: 1
    parallel: 1
    total_steps: 10000000
  algo_cfgs:
    steps_per_epoch: 20000
    update_iters: 40
    batch_size: 64
    target_kl: 0.02
    entropy_coef: 0.0
    reward_normalize: False
    cost_normalize: False
    obs_normalize: True
    kl_early_stop: True
    use_max_grad_norm: True
    max_grad_norm: 40.0
    use_critic_norm: True
    critic_norm_coef: 0.001
    gamma: 0.99
    cost_gamma: 0.99
    lam: 0.95
    lam_c: 0.95
    clip: 0.2
    adv_estimation_method: gae
    standardized_rew_adv: True
    standardized_cost_adv: True
    penalty_coef: 0.0
    use_cost: True
  logger_cfgs:
    use_wandb: False
    wandb_project: omnisafe
    use_tensorboard: True
    save_model_freq: 100
    log_dir: "./runs"
    window_lens: 100
  model_cfgs:
    weight_initialization_mode: "kaiming_uniform"
    actor_type: gaussian_learning
    linear_lr_decay: True
    exploration_noise_anneal: False
    std_range: [0.5, 0.1]
    actor:
      hidden_sizes: [64, 64]
      activation: tanh
      lr: 0.0003
    critic:
      hidden_sizes: [64, 64]
      activation: tanh
      lr: 0.0003
  lagrange_cfgs:
    cost_limit: 25.0
    lagrangian_multiplier_init: 0.001
    lambda_lr: 0.035
    lambda_optimizer: "Adam"
  env_cfgs: {}
  semantic_cfgs:
    enable: False
    capture_interval: 4
    frame_size: 224
    model_name: "openai/clip-vit-base-patch16"
    shaping_enable: False
    risk_enable: False
    modulation_enable: False
    beta_start: 0.05
    beta_end_step_fraction: 0.4
    risk_horizon: 64
    discount: 0.99
    alpha_modulation: 2.0
    threshold_percentile: 60
    slope: 5.0
    window_size: 2048
    safe_prompts: ["clear path to goal", "centered trajectory", "ample clearance"]
    unsafe_prompts: ["imminent collision", "tight near-obstacle turn", "risky close pass"]
